* Security
1. Creating a new (non-root) user
    #+BEGIN_SRC shell
      sudo adduser someusername
      sudo groupadd sudo                   # create group sudo
      sudo usermod -a -G sudo someusername # for redhat: sudo usermod -a -G wheel someusername
    #+END_SRC
2. Allowing this user to use “sudo” for administrative privileges
    =/etc/sudoers=
    #+BEGIN_SRC
     # [bracket] around each section being discussed.
     # We can also set rules for groups. Group rules are prefixed with a %
     [root] ALL=(ALL:ALL) ALL # This applies to user root
     root [ALL]=(ALL:ALL) ALL # This rule applies to all user root logged in from all hosts
     root ALL=([ALL]:ALL) ALL # User root can run commands as all users
     root ALL=(ALL:[ALL]) ALL # User root can run commands as all groups
     root ALL=(ALL:ALL) [ALL] # These rules apply to all commands
    #+END_SRC

    The following entry will allow user vagrant to run all commands with
    sudo without specifying a password: Editing /etc/sudoers via visudo
    #+BEGIN_SRC
     vagrant ALL=(ALL:ALL) NOPASSWD:ALL
    #+END_SRC

    Let’s give the group “admin” the ability to run ‘sudo mkdir’
    without a password, but require a password to run sudo rm:
    #+BEGIN_SRC
    %admin ALL NOPASSWD:/bin/mkdir, PASSWD:/bin/rm
    #+END_SRC
3. Stopping user =root= from remotely logging in via SSH
   File: =/etc/ssh/sshd_config=
    #+BEGIN_SRC
    PermitRootLogin no
    #+END_SRC
4. Configuring SSH to change the port and add other restrictions
    We’re allowed to assign ports between 1024 and 65536. To change
    the SSH port, change the Port option in the same
    File: =/etc/ssh/sshd_config=
    #+BEGIN_SRC
    Port 1234
    #+END_SRC
    and to connect
    #+BEGIN_SRC
     ssh -p 1234 user@hostname
    #+END_SRC
    Also possible to restrict by user and group
    #+BEGIN_SRC
     # Can define multiple users,
     # separated by a space
     AllowUsers someusername anotherusername
     AllowGroups sudo canssh
    #+END_SRC

5. Creating an SSH key on our local computer for logging in as our new
   user

   #+BEGIN_SRC
   ssh-keygen -t rsa -b 4096 -C your@email.com -f id_myidentity

   # -t rsa - Create an RSA type key pair5.
   # -b 4096 - Use 4096 bit encryption. 2048 is “usually sufficient”, but I go higher.
   # -C your@email.com - Keys can have comments. Often a user’s identity goes here as a comment, such as their name or email address
   # -f id_myidentity - The name of the SSH identity files created. The two files would be id_- myidentity and id_myidentity.pub in this example.
   #+END_SRC

   append =id_myidentity.pub= to =~/.ssh/authorized_keys= one can do this manually or like
   #+BEGIN_SRC
   ssh-copy-id -p 3022 -i ~/.ssh/id_rsa.pub me@104.236.54.306
   #+END_SRC

   NOTE: You may also need to set some permissions of your =.ssh=
   directory and =authorized_keys= file on your server.

   #+BEGIN_SRC
   ssh -i ~/.ssh/my_identity -o "IdentitiesOnly yes" someusername@my-server.com
   #+END_SRC

6. Turning off password-based authentication, so we must use an SSH
   key to access the server
   #+BEGIN_SRC
   PasswordAuthentication no
   #+END_SRC

** Setting Up the Firewall

- Configuring iptables involves setting up the list of rules that
  check network traffic. The rules are checked whenever a piece of
  data enters or leaves the server over a network. If the iptables
  rules allows the traffic type, it goes through. If traffic is not
  allowed, the data packet is dropped or rejected.

- Rejecting data lets the other end know data was not allowed
  through. Dropping the data behaves like a blackhole, where no
  response is made.

*** =iptables=
It's an interface to the netfilter that is built-in to the Linux Kernel.

The tables are made up of a set of predefined chains, and the chains
contain rules which are traversed in order.  Each rule consists of a
predicate of potential matches and a corresponding action (called a
target) which is executed if the predicate is true; i.e. the
conditions are matched. iptables is the user utility which allows you
to work with these chains/rules.

In the vast majority of use cases you won't need to use the raw,
mangle, or security tables at all. Consequently, the following chart
depicts a simplified network packet flow through iptables:

    #+BEGIN_SRC
                                   XXXXXXXXXXXXXXXXXX
                                 XXX     Network    XXX
                                   XXXXXXXXXXXXXXXXXX
                                           +
                                           |
                                           v
     +-------------+              +------------------+
     |table: filter| <---+        | table: nat       |
     |chain: INPUT |     |        | chain: PREROUTING|
     +-----+-------+     |        +--------+---------+
           |             |                 |
           v             |                 v
     [local process]     |           ****************          +--------------+
           |             +---------+ Routing decision +------> |table: filter |
           v                         ****************          |chain: FORWARD|
    ****************                                           +------+-------+
    Routing decision                                                  |
    ****************                                                  |
           |                                                          |
           v                        ****************                  |
    +-------------+       +------>  Routing decision  <---------------+
    |table: nat   |       |         ****************
    |chain: OUTPUT|       |               +
    +-----+-------+       |               |
          |               |               v
          v               |      +-------------------+
    +--------------+      |      | table: nat        |
    |table: filter | +----+      | chain: POSTROUTING|
    |chain: OUTPUT |             +--------+----------+
    +--------------+                      |
                                          v
                                   XXXXXXXXXXXXXXXXXX
                                 XXX    Network     XXX
                                   XXXXXXXXXXXXXXXXXX

    #+END_SRC

- Tables:
  + iptables contains five tables:
    * =raw= is used only for configuring packets so that they are exempt from connection tracking.
    * =filter= is the default table, and is where all the actions typically associated with a firewall take place.
    * =nat= is used for network address translation (e.g. port forwarding).
    * =mangle= is used for specialized packet alterations.
    * =security= is used for Mandatory Access Control networking rules (e.g. SELinux -- see this article for more details).
  + In most common use cases you will only use two of these: =filter=
    and =nat=. The other tables are aimed at complex configurations
    involving multiple routers and routing decisions

- Chains
  Tables consist of chains, which are lists of rules which are followed
  in order. The default table, =filter=, contains three built-in chains:
  =INPUT=, =OUTPUT= and =FORWARD= which are activated at different points of
  the packet filtering process, as illustrated in the flow chart. The
  =nat= table includes =PREROUTING=, =POSTROUTING=, and =OUTPUT= chains.

- Rules Packet filtering is based on rules, which are specified by
  multiple matches (conditions the packet must satisfy so that the
  rule can be applied), and one target (action taken when the packet
  matches all conditions). The typical things a rule might match on
  are what interface the packet came in on (e.g eth0 or eth1), what
  type of packet it is (ICMP, TCP, or UDP), or the destination port of
  the packet. Targets are specified using the -j or --jump
  option. Targets can be either user-defined chains (i.e. if these
  conditions are matched, jump to the following user-defined chain and
  continue processing there), one of the special built-in targets, or
  a target extension. Built-in targets are ACCEPT, DROP, QUEUE and
  RETURN, target extensions are, for example, REJECT and LOG. If the
  target is a built-in target, the fate of the packet is decided
  immediately and processing of the packet in current table is
  stopped. If the target is a user-defined chain and the fate of the
  packet is not decided by this second chain, it will be filtered
  against the remaining rules of the original chain. Target extensions
  can be either terminating (as built-in targets) or non-terminating
  (as user-defined chains)

- Modules
  There are many modules which can be used to extend iptables such as
  connlimit, conntrack, limit and recent. These modules add extra
  functionality to allow complex filtering rules.

- What can filtering protect againt?
  + Source address spoofing
  + Useful information revealed in response to port scans
  + Malformed broadcast packetss used to identify UNIX systems
  + Some denial-of-sevice attacks
  + Source-routed packets
  + some forms of fragmentation bombs
  + Local mistakes that affect remote sites
  + Access to private LAN services

- Logging

Create the chain with:

#+BEGIN_SRC shell
iptables -N logdrop
#+END_SRC

And add the following rules to the newly created chain:

#+BEGIN_SRC shell
iptables -A logdrop -m limit --limit 5/m --limit-burst 10 -j LOG
iptables -A logdrop -j DROP
#+END_SRC

Limiting log rate

The above logdrop chain uses the limit module to prevent the iptables
log from growing too large or causing needless hard drive
writes. Without limiting an erroneously configured service trying to
connect, or an attacker, could fill the drive (or at least the /var
partition) by causing writes to the iptables log.

The limit module is called with -m limit. You can then use --limit to
set an average rate and --limit-burst to set an initial burst rate. In
the logdrop example above:

#+BEGIN_SRC shell
iptables -A logdrop -m limit --limit 5/m --limit-burst 10 -j LOG
#+END_SRC

appends a rule which will log all packets that pass through it. The
first 10 consecutive packets will be logged, and from then on only 5
packets per minute will be logged. The "limit burst" count is reset
every time the "limit rate" is not broken, i.e. logging activity
returns to normal automatically.  Viewing logged packets

Logged packets are visible as kernel messages in the systemd journal.

To view all packets that were logged since the machine was last booted:

#+BEGIN_SRC shell
journalctl -k | grep "IN=.*OUT=.*" | less
#+END_SRC

Now whenever we want to drop a packet and log this event, we just jump
to the =logdrop= chain, for example:

#+BEGIN_SRC shell
iptables -A INPUT -m conntrack --ctstate INVALID -j logdrop
#+END_SRC

=iptables= syntax
#+BEGIN_SRC shell
# flush all the firewall rules
iptables --flush

# delete rulenum 6 of INPUT chain in filter (default) table.
sudo iptables -D INPUT 6

# port range. Insert a new rule in INPUT chain of filter (default) table for the given port range of UDP Protocal
sudo iptables -I INPUT 6 -p udp --dport 60000:61000 -j ACCEPT

# to accept current/established connections:
sudo iptables -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT


# append to INPUT chain, -i for interface, -j for --jump `target`
# lo for loopback
iptables -A INPUT -i lo -j ACCEPT  # always necessary

# Output Rules as Commands
iptables -S

# append to OUTPUT chain, -o for outgoing interface,
iptables -A OUTPUT -o lo -j ACCEPT # always necessary


# to have persistent iptables
sudo apt install -y iptables-persistent
# Start the service
sudo service iptables-persistent start

# Output rules to a file called "iptables-backup.rules"
sudo iptables-save > iptables-backup.rules

# Save current rules to iptables rules file
sudo iptables-save | sudo tee /etc/iptables/rules.v4

# Restore rules from our backup file
sudo iptables-restore < iptables-backup.rules


# change default policy
iptables --policy INPUT DROP
iptables --policy OUTPUT DROP
iptables --policy FORWARD DROP
#+END_SRC

- iptables targets
  + =ACCEPT=: let the packet through
  + =DROP=: drop the packet quietly
  + =REJECT=: return icmp-port-unreachable
  + =RETURN= target. The “RETURN” target simply tells iptables to send
    the traffic back to where it came from - the INPUT chain in this
    case. There it will be analyzed by the rest of rules in the INPUT
    chain.

- =iptables= example
  + Drop FTP Request:
    #+BEGIN_SRC shell
    SERVER_IP="192.168.0.3"
    iptables -I INPUT -i eth0 -p tcp -s 0/0 -d $SERVER_IP --dport 21 -j DROP
    # -I: insert
    # INPUT chain
    # -i eth0: interface eth0
    # -p tcp: protocol tcp because FTP runs over TCP
    # -s 0/0: source address, 0/0 means any source address
    # -d $SERVER_IP: destination address
    # --dport 21: on port 21
    #+END_SRC
  + Allow SSH Connections
    #+BEGIN_SRC shell
    iptables -I INPUT -i eth0 -p tcp -s 0/0 -d $SERVER_IP --dport 22 -j ACCEPT
    #+END_SRC
  + Enable or allow ICMP ping incoming client request
    #+BEGIN_SRC shell
    iptables -I INPUT -i eth0 -p icmp -icmp-type 8 -s 0/0 -d $SERVER_IP -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT
    # -p icmp: our protocal is icmp
    # -icmp-type 8: type 8 is echo-request (that is to say a ping)
    # -m, --match match (from man page)
    #   Specifies a match to use, that is, an extension module that tests for a specific property. The set of matches make up the condition under which a target is invoked. Matches are evaluated  first
    #   to last as specified on the command line and work in short-circuit fashion, i.e. if one extension yields false, evaluation will stop.
    #+END_SRC

    #+BEGIN_SRC shell
    iptables -I OUTPUT -i eth0 -p icmp -icmp-type 0 -s $SERVER_IP -d 0/0 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT
    # note the -s and -d is swapped comparing to previous rule
    #+END_SRC
  + Disable outgoing ICMP request
    #+BEGIN_SRC shell
    iptables -A OUTPUT -p icmp --icmp-type echo-request -j DROP
    # or
    iptables -A OUTPUT -p icmp --icmp-type 8 -j DROP
    # NOTE: No -s and no -d means apply to all source and destination addresses
    #+END_SRC
  + Stateful Packet Inspection
    #+BEGIN_SRC shell
    iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
    iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
    #+END_SRC
** Fail2Ban
Fail2Ban monitors for instrusion attempts on your server. It uses the
iptables firewall to ban specific hosts if they meet a configured
threshold of invalid attempts.

Here’s the explanation from the website;

Fail2Ban scans log files (e.g. =/var/log/apache/error_log=) and bans
IPs that show the malicious signs – too many password failures,
seeking for exploits, etc. Generally Fail2Ban is then used to update
firewall rules to reject the IP addresses for a specified amount of
time, although any arbitrary other action (e.g. sending an email)
could also be configured. Out of the box Fail2Ban comes with filters
for various services (apache, courier, ssh, etc).

Fail2Ban creates a new iptables chain per jail it monitors. For SSH,
Fail2Ban will create a chain called “Fail2Ban-ssh”. This chain (and
others it creates) is used early in the iptables INPUT chain, so it
gets checked first.

#+BEGIN_SRC
 sudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local
#+END_SRC

Files in =jail.d= can overwrite existing configurations and add
additional configurations. However, we’ll simply use the =jail.local=
file for our purposes.

** Automatic Security Updates
#+BEGIN_SRC
sudo apt install -y unattended-upgrades

#+END_SRC
Make sure ="${distro_id}:${distro_codename}-security";= is enabled in
=/etc/apt/apt.conf.d/50unattended-upgrades=
Disable Automatic-Reboot
#+BEGIN_SRC
Unattended-Upgrade::Automatic-Reboot "false";
#+END_SRC
Finally, create or edit the =/etc/apt/apt.conf.d/20auto-upgrades= file
and ensure these lines are present:
#+BEGIN_SRC
APT::Periodic::Update-Package-Lists "1";
APT::Periodic::Unattended-Upgrade "1";
#+END_SRC

This will run once at set intervals. “Periodic” items are set to run
once per day via the daily cron.  If you’re curious, you can find that
configured in the =/etc/cron.daily/apt-compat= file which in turn runs
=/usr/lib/apt/apt.systemd.daily= which in turn runs various apt
settings, such as our unattended upgrades.

Upgrade information is logged within the
=/var/log/unattended-upgrades= directory.

* Package Managers
#+BEGIN_SRC
add-apt-repository -y ppa:namespace-name/repo-name
apt -n search redis
apt show redis-server
apt policy redis-server
#+END_SRC

* User Management
Every user created by default belongs to a user and group of the same
name. Users can belong to one primary group, and then can be added to
many other secondary groups.  The primary group is usually what
files/directories are assigned when a user creates a new file or
directory. (Their username is of course the user assigned to those
same files/directories).

/etc/passwd
=sfh:x:1000:1000:,,,:/home/sfh:/usr/bin/zsh=
- User
- Password (“x” meaning the user has an encrypted password)
- User ID (UID)
- Group ID (GID)
- User Information (extraneous notes)
- Home Directory
- Command/Shell used by the user

#+BEGIN_SRC
adduser deploye
su - deployer
usermod -g www-data deployer # Assign group www-data (primary)
chgrp -R www-data /var/www
chmod -R g+rwx /var/www
#+END_SRC
*** umask
Many users have a umask of 022. These numbers follow the User, Group
and Other scheme. The series 022 means:

=0= - User can read, write and execute
=2= - Group can read, execute
=2= - Other can read, execute

Here’s what octal values we can use for each of the three numbers:
=0= - read, write and execute
=1= - read and write
=2= - read and execute
=3= - read only
=4= - write and execute
=5= - write only
=6= - execute only
=7= - no permissions

*** SUID, GUID and Sticky Bit
**** SUID
NUMERICALLY: 4
Execute with UID of the file, instead of the UID of the current user.
    - SUID Stands for Super User ID
    - SUID should be applied to a binary file or to a script
    - SUID alphabetically identified by letter “s” and Numerically
      Identified by “4“.
    -
**** GUID
NUMERICALLY: 2
On Files: Execute with GID of the file, instead of the GID of the current user.

On Directories: we need new files/directories to take on the group of
their parent directories. We can do this with the “group id bit”.
We’ll use a familiar command to do that:

#+BEGIN_SRC
sudo chmod g+s /var/www # Set group id bit of directory /var/www
#+END_SRC
**** Sticky bit
NUMERICALLY: 1
On Directories: As in /tmp where any user can store files in /tmp but
only the owner of the file has the rights to modify or delete the file
from /tmp

* HTTP, Web Servers and Web Sites
If a web server is hosting multiple web sites, how does the server
route incoming requests to the correct web site?

It reads the HTTP request’s Host header. If the Host header is not
present or doesn’t match a defined site, the web server routes the
request to a default site.
** DNS (Domain Name System)
The Host header can inform our web server what website a client requested.
However, how does a domain used in a user’s browser reach our server in the first place?
[[https://aws.amazon.com/route53/what-is-dns/][What is DNS]]?

*** CNAME
A CNAME or Canonical Name record links an alias name to another true
or canonical domain name. For instance, www.example.com might link to
example.com.

*** A Record
An A or Address record (also known as a host record) links a domain to
the physical IP address of a computer hosting that domain's services.

*** NS Record
Name server (NS) records determine which servers will communicate DNS
information for a domain. Generally, you have primary and secondary
name server records for your domain.

*** MX Record
Mail Exchange (MX) records direct a domain's email to the servers
hosting the domain's user accounts.

*** Time To Live (TTL)
The TTL is a value in a DNS record that determines the number of
seconds before subsequent changes to the record go into effect. Each
of your domain's DNS records, such as an MX record, CNAME record, and
so on, has a TTL value. A record's current TTL determines how long it
will take any change you make now to go into effect. Changes to a
record that has a TTL of 86400 seconds, for example, will take up to
24 hours to go into effect.

** Hosts

The hosts file is providing the service of telling our computer what
IP address to resolve to when the domain is used.

We can also use xip.io

Web servers look for the Host header in an HTTP request to map the
request to a configured website.

** Three Actors
Hosting a web application requires the orchestration of three actors:

*** The Application
*** The Gateway (Like CGI?)
Some common functionality of gateways include:
   + Listen for requests (HTTP, FastCGI, uWSGI and more)
   + Translate requests to application code
   + Spawn multiple processes and/or threads of applications
   + Monitor spawned processes
   + Load balance requests between processes
   + Reporting/logging

Consider a gateway receiving a request meant for a Python
application. The gateway will translate the request into a
WSGI-compliant request.

PHP-FPM, the gateway for PHP, is an implementation of FastCGI. It will
listen for FastCGI requests from a web server.

Many gateways can accept HTTP requests directly. uWSGI, Gunicorn, and
Unicorn are examples of such gateways.  Other protocols are also often
supported. For example, uWSGI will accept HTTP, FastCGI and uwsgi
(lowercase, the protocol) requests.

No matter what protocol is used, gateways can accept a request and
translate it to speak a web application’s “language”.

Applications built in such languages can skip the use of gateways. In
that scenario, a web server will send HTTP requests directly to the
application.  Such applications can still benefit from the use of a
gateway. For example, NodeJS applications.  Node’s asynchronous model
allows it to run efficiently as a single-process. However, you may
want to use multiple processes on multi-core servers.  A NodeJS
gateway such as =PM2= could manage multiple processes. This would
allow for more concurrent application requests to be handled.

*** The Web Server
Web servers excel at serving requested files, but usually serve other
purposes as well.

Popular web-server features include:
    - Hosting multiple sites
    - Serving static files
    - Hosting Web Applications
    - Proxying requests to other processes
    - Load balancing
    - HTTP caching
    - Streaming media

As mentioned, a web server will translate an HTTP request to something
a gateway can understand.  Gateways listen for requests using various
protocols.

Some gateways can listen for HTTP connections. In this case, the web
server can relay the HTTP request to the gateway directly.

Other gateways listen for FastCGI or uwsgi connections. Web servers
which support these protocols must translate an HTTP request to those
protocols.

Nginx and Apache can both “speak” HTTP, uwsgi, and FastCGI. These web
servers will accept an HTTP request and relay them to a gateway in
whichever protocol the gateway needs.

Most servers will only have one public IP address assigned to
them. That’s not always the case, however. One common reason to add
extra IP addresses is when using an SSL certificate. In some
situations, an IP address must be unique per domain under an SSL
certificate. If a server has multiple sites using their own SSL
certificates, they’ll need more than one IP address.

To host a web application, a web server can accept an HTTP request and
pass it (proxy it) off to a “gateway”. The gateway handles converting
the request into something an application can understand.

These gateways are various implementations and flavors of a “CGI”s - a
Common Gateway Interfaces.

For example, many Python applications use the uWSGI36 gateway. Apache
will “proxy” a request to the gateway. In turn, the uWSGI gateway
passes the request to the Python application.

PHP, when not directly loaded by Apache, can use the PHP-FPM
gateway. FPM is an implementation of the FastCGI gateway, which is a
very common protocol.

Apache can also proxy to web applications over HTTP. This is popular
when proxying requests to applications listening on HTTP. NodeJS and
Golang are two languages that can listen for HTTP connections
directly.

Gunicorn and Unicorn are two popular gateways which can communicate
over HTTP as well. These can be used to serve Python and Ruby
applications, respectively.

In the next sections, we’ll discuss how Apache can talk to
applications using HTTP, FastCGI and WSGI gateways.

Note that gateways are commonly tied to specific languages, but some
are not!

Apache can handle requests for static assets. This frees the
application from wasting resources on static assets.

Apache can send requests to pools of resources of the
application. Instead of one running NodeJS process, picture 3-4
running! Apache can send requests to each of them. This would
substantially increase the number of requests the application could
simultaneously handle.  This essentially is load balancing.

Some gateways monitor and manage multiple application processes for
us. A gateway will expose one HTTP listener for Apache to send
requests to. The gateway would then be responsible for sending
requests to each running process. Some gateways can dynamically spin
up and destroy running application processes.

Apache has a module system that can determine how requests are
processed. These are called Multi- Processing Modules (MPM). They are
responsible for communicating over a network, accepting requests, and
dispatching child processes/threads to handle requests.  There are
three MPM modules to consider:

- MPM Prefork
- MPM Worker
- MPM Event

- *MPM Prefork*: MPM Prefork is usually the default MPM used in
  Apache. It does not use threads. An entire process is dedicated to
  each HTTP request.  The default may change depending on how you
  install Apache. For example, the repository we use,
  ppa:ondrej/apache2, enables Event as the default MPM.  If you
  install the php5 module, this is automatically changed to MPM
  Prefork. You can see which is installed on Debian/Ubuntu servers at
  /etc/apache/mods-enabled/ and see which mpm_*.conf file is present.
  Because each process handles only one request, Prefork is slightly
  quicker than a threaded module.  There’s no processing time spent
  creating and tracking threads.  While using processes is a little
  faster, they can eat up CPU and memory in a situation where there is
  lots of simultaneous requests. A threaded module will be able to
  handle more concurrent requests.

- *MPM Worker*: MPM Worker uses threading. Each process can spawn
  multiple threads. Threads are much cheaper to create than processes,
  and so fewer expensive processes need to be created and
  managed. This helps Apache handle more concurrent requests by
  reducing the overall memory needed to handle each request.  With MPM
  Worker, the processes spawn threads to handle incoming HTTP
  requests. To be precise, Worker uses one thread per HTTP
  connection. Multiple HTTP requests can be made per connection.  A
  thread will handle multiple requests until a connection is
  closed. As a request is completed and a connection closed, the
  thread opens up to accept the next connection and handle its
  requests.

- *MPM Event*: MPM Event is the newest processing module. It works just
  like Worker, except it dedicates a thread to each HTTP request. A
  thread is created per HTTP request, rather than per connection.
  This means that a thread will free up when the HTTP request is
  complete, rather than when the connection is closed. Connections are
  managed within the parent process rather than the threads.  MPM
  Event is better for applications with relatively long-lasting
  requests (long Keep-Alive timeouts). With MPM Worker, each
  long-running connection would use a whole thread. With Event,
  threads don’t need to be taken up by connections which may or may
  not be sending any data at the moment. A process can use a thread
  only when a new request comes from the connection.  An application
  using server-push, long-polling or web sockets are good use cases
  for employing MPM Event.
  If a connection is made using SSL or TLS, MPM Event defaults back to
  working just like MPM Worker. It will handle a connection per
  thread.

**** Nginx
Nginx is similar to NodeJS, HAProxy, and other “web scale”
technologies (put in quotes, only a tad sarcastisically). Nginx runs
as an evented, single process. It manages requests
asynchronously. This helps Nginx work with a large number of
concurrent connections while using a stable and relatively low amount
of memory. Actually Nginx can use a few processes. A typical setup with
Nginx will spawn as many processes as there are CPU cores on the
server.

Apache, as we learned, spawns processes or threads for each
connection. Its synchronous manner means that processes and threads
pause (“block”) while performing slower tasks.  Examples of such tasks
are reading from the file system or performing network
operations. This means that Apache processes are “blocking”; We must
wait for them to finish their task before moving onto the next one.

While Apache spawns many processes and threads, Nginx spawns very few
processes (“workers”).  Each process is single-threaded. Nginx workers
accept requests from a shared socket and execute them inside of an
efficient run-loop. Nginx is asynchronous, evented and
non-blocking. It is free to accomplish other tasks while waiting for
slow tasks such as file I/O or network operations to finish.

Each Nginx worker can process thousands of simultaneous connections. It
avoids the overhead of constantly creating, tracking and destroying
new processes/threads. This is much more memory and CPU efficient.

Nginx can act as a:
- Web Server
- Reverse Proxy (“Application Proxy”)
    + A proxy server is a go between or intermediary server that
      forwards requests for content from multiple clients to different
      servers across the Internet. A reverse proxy server is a type of
      proxy server that typically sits behind the firewall in a
      private network and directs client requests to the appropriate
      backend server. A reverse proxy provides an additional level of
      abstraction and control to ensure the smooth flow of network
      traffic between clients and servers.

      Common uses for a reverse proxy server include:

      Load balancing – A reverse proxy server can act as a “traffic
      cop,” sitting in front of your backend servers and distributing
      client requests across a group of servers in a manner that
      maximizes speed and capacity utilization while ensuring no one
      server is overloaded, which can degrade performance. If a server
      goes down, the load balancer redirects traffic to the remaining
      online servers.  Web acceleration – Reverse proxies can compress
      inbound and outbound data, as well as cache commonly requested
      content, both of which speed up the flow of traffic between
      clients and servers. They can also perform additional tasks such
      as SSL encryption to take load off of your web servers, thereby
      boosting their performance.  Security and anonymity – By
      intercepting requests headed for your backend servers, a reverse
      proxy server protects their identities and acts as an additional
      defense against security attacks. It also ensures that multiple
      servers can be accessed from a single record locator or URL
      regardless of the structure of your local area network.

- Content Caching (“Web Cache”)
- Load Balancer
  + TCP Load Balancing as of 1.9.4
- SSL Terminator

***** Configuration
In Ubuntu, Nginx follows the usual scheme for configuration. Let’s
look at some files and directories in the /etc/nginx directory:

#+BEGIN_SRC
/etc/nginx/conf.d
/etc/nginx/sites-available
/etc/nginx/sites-enabled
/etc/nginx/nginx.conf
#+END_SRC

Configurations can be enabled by symlinking a file from
=sites-available= to the =sites-enabled= directory

- Main nginx config is =/etc/nginx/nginx.conf=
- Any =.conf= file included in =/etc/nginx/conf.d= will be included and
  enabled.
- Like Apache, Nginx has the concept of Virtual Hosts, which we’ll
  just call “servers” in context of Nginx.

basic nginx server
#+BEGIN_SRC nginx
  server {
      listen 80 default_server; # defines itself as the default server for requests on port 80. If no Host HTTP header matches a configured server, then Nginx will default back to this default site.
      listen [::]:80 default_server; # We also listen on port 80 of an ipv6 interface, if it is enabled on the server
      root /var/www/html; # This is where the web files are pulled from.
      index index.html index.htm index.nginx-debian.html; # The index directive defines which files are to be attempted to be read if no file is specified
      server_name _; # The hostname that Nginx should use to match the Host header with to match to this server. Since this is a default server (via default_server), currently this site will load if no other host is matched.
  }

  # Nginx can use the location block along with a file path or regex
  # pattern to match URL’s or files and handle them differently.
  # Here we see any location is grabbed. Then the try_files directive
  # will attempt to find a file in the order of the given patterns. By
  # default, this tries to use the explicit URL to find a file, followed by a
  # directory name, and lastly responds with a 404 if no matching file
  # or directory is found.
  location / {                    # This captures the URI “/” and any sub-URI (sub-directory). In other words, this location block applies to all URIs.
      try_files $uri $uri/ =404;  # The use of try_files is good for handling static content. It tries to find the URI as a file or directory in the order of the defined variables.
  }
#+end_src

+ The =location= directive is very important. It helps determine how
  files and URI’s are handled Nginx.
+ =try_files $uri $uri/ =404;=
  1) First try the URI given to see if a matching file can be found on the server. This is relative to the root path.
  2) Failing to find a file, try the URI as a directory on the server. This is relative to the root path.
  3) Failing to find a file or a directory, respond with a 404 error.
+ Example
  #+BEGIN_SRC nginx
  location = /favicon.ico { log_not_found off; access_log off; }
  location = /robots.txt { log_not_found off; access_log off; }
  #+END_SRC

  The above two directives will turn off 404 error logging and any
  access log information on these two files.
+ Blocking Access to Files
  #+BEGIN_SRC nginx
  location ~ /\. {
       deny all;
       access_log off;
       log_not_found off;
   }
  #+END_SRC
  This turns off the access log and 404 error logging for “dot
  files”. If the files or directories exist on the server, Nginx
  will deny access to them.
+ Handling Files by Extension
  #+BEGIN_SRC nginx
  location ~ \.(js|css|png|jpg|gif|swf|ico|pdf|mov|fla|zip|rar)$ { # The tilde instructs nginx to perform a case-sensitive regular expression match, instead of a straight string comparison.
      try_files $uri =404;
  }
  #+END_SRC
  This uses regular expressions to match files =.js=, =.css= and the
  others listed above. The above uses =try_files= for the exact file
  name but doesn’t attempt to match them as a directory.

  We can reduce the risk of unwanted PHP files being processed by
  explicitly specifying which PHP files can be run:

  #+BEGIN_SRC nginx
  location ~ ^/(app|app_dev|config)\.php(/|$) {
      ...magic here...
  }
  #+END_SRC
+ Pretty URL’s (Hiding ‘index.php’)
  #+BEGIN_SRC nginx
  # With a PHP application, this
  # becomes our default `location {}` block
  location / {
      try_files $uri $uri/ /index.php$is_args$args;
  }
  #+END_SRC
+ Redirect www to non-www

  #+BEGIN_SRC nginx
   server {
       server_name *.example.com;
       return 301 $scheme://example.com$request_uri;
   }

   server {
       server_name example.com;
       ...
   }
  #+END_SRC
+ Redirect non-www to www
  #+BEGIN_SRC nginx
  server {
      listen 80;
      server_name example.com;
      return 301 http://www.example.com$request_uri;
  }
  server {
      listen 80;
      server_name www.example.com;
      ...
  }
  #+END_SRC
+ Forcing SSL
  #+BEGIN_SRC nginx
   server {
       listen 80;
       server_name example.com www.example.com;
       return 301 https://example.com$request_uri;
   }
   server {
       listen 443 ssl;
       server_name example.com;
       ssl on;
       # Other SSL directives, covered later
       # ...
   }
  #+END_SRC
+ Wildcard Subdomains and Document Root
  #+BEGIN_SRC nginx
  server {
      listen 80 default_server;
      server_name ~^(.*)\.local\.dev$;
      set $file_path $1;
      root /var/www/$file_path/public
      location / { ... }
  }
  #+END_SRC
  We’re using regular expressions in the server_name directive. This
  matches any subdomain and captures the subdomain. The subdomain is
  available via the $1 variable. The $1 variable is the result of
  the first capture group found in the regular expression
  =∼ˆ(.*)\.local\.dev$=.  We then use the $1 variable and map it to a
  variable called $file_path. Lastly, we append =$file_path= to the
  root directive to make up part of our document root. This will
  dynamically change the document root based on the subdomain used.
  Each subdomain will automatically map to our project directories!
+ Integration with Web Applications
  Typically a web server will accept a request and pass it off to a
  “gateway”. Gateways then translate and pass the request off to a
  coded application. Gateways are various implementations and
  flavors of a “CGI”s - a Common Gateway Interfaces.

  * HTTP Proxy
    #+BEGIN_SRC nginx
    location /api {
      include proxy_params;
      proxy_pass http://127.0.0.1:9000;
    }
    #+END_SRC
    We included the /etc/nginx/proxy_parms file. This file contains
    some sensible defaults to use when proxying requests for another
    service. Here’s what that file does:

    - It sets the Host header to the requests original Host
    - It adds a X-Read-IP header to the IP address of the original request
    - It adds a X-Forwarded-For header
    - It adds a X-Forwared-Proto header

    Nginx will return to the client whatever the backend server returns.

    A Unix socket version of the same proxy pass might look like
    this: (NOTE: if =foobar.sock= permission is important)
    #+BEGIN_SRC nginx
    location /api {
         include proxy_params;
         proxy_pass http://unix:/tmp/foobar.sock:/;
    }
    #+END_SRC
  * Multiple Backends

    Nginx can proxy off to multiple HTTP backends. In fact, this is
    Nginx’s load balancing!

    #+BEGIN_SRC nginx
      upstream my_app {
          zone backend 64k;
          least_conn; # LB algorithms
          server 127.0.0.1:9000 max_fails=3 fail_timeout=30s;
          server 127.0.0.1:9001 max_fails=3 fail_timeout=30s;
      }
      server {
          listen 80 default_server;
          listen [::]:80 default_server ipv6only=on;
          root /usr/share/nginx/html;
          index index.html index.htm;
          server_name localhost
          charset utf-8;

          location /static {
              try_files $uri $uri/ =404;
          }

          location / {
              include proxy_params;
              proxy_pass http://my_app/;
          }
      }
    #+END_SRC
  * FastCGI
    - php-fmp
        #+BEGIN_SRC nginx
          server {
              listen 80 default_server;
              listen [::]:80 default_server ipv6only=on;
              root /usr/share/nginx/html;
              index index.html index.htm;
              server_name localhost
              charset utf-8;

              location / {
                  try_files $uri $uri/ /index.php$is_args$args; # will pass requests off to PHP last if no directory or static file is found to serve the request.
              }

              location ~ \.php$ {
                  include snippets/fastcgi-php.conf;

                  fastcgi_pass 127.0.0.1:9000; # when on a unix socket => fastcgi_pass unix:/run/php/php7.2-fpm.sock;
                  fastcgi_param ENV development;
              }
          }
        #+END_SRC

    - other CGIs
      #+BEGIN_SRC nginx
      location / {
        try_files $uri $uri/ @proxy;
      }

      location @proxy {
          include fastcgi_params;
          fastcgi_pass 127.0.0.1:9000;
          fastcgi_param ENV development;
      }
      #+END_SRC
      This attempts to find the URI as an existing file or
      directory. If they don’t exist, it jumps the request to the
      =@proxy= location block. This will then proxy the request to the
      back-end server (application) configured.
  * uWSGI
    #+BEGIN_SRC nginx
      location / {
          try_files $uri $uri/ @proxy;
      }
      location @proxy {
          include uwsgi_params;
          uwsgi_pass 127.0.0.1:9000;
          uwsgi_param ENV productionmaybe;
      }
    #+END_SRC


* PHP
