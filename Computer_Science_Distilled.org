Mostly from the “Computer Science Distilled”, Wikipedia and “Data
Structures and Performance” by University of California San Diego and
some of FrontEndMasters courses.

* Flowcharts
  + Write states and instruction steps inside rectangles.
  + Write decision steps, where the process may go different ways, inside diamonds.
  + Never mix an instruction step with a decision step.
  + Connect sequential steps with arrows.
  + Mark the start and end of the process.

* Mathematical Models
  + A *model* is a set of concepts that represents a problem and its characteristics.

* Boolean Algebra
  + *Associativity*
    * Parentheses are irrelevant for sequences of =AND= or =OR=
      operations. As sequences of sums or multiplications in
      elementary algebra, they can be calculated in any order.
      #+BEGIN_SRC
          A AND (B AND C) = (A AND B) AND C
          A OR (B OR C) = (A OR B) OR C
      #+END_SRC
  + *Distributively*
    * In elementary algebra we factor multiplicative terms from sums:
      a × (b + c) = (a × b) + (a × c). Likewise in logic, ANDing
      after an OR is equivalent to ORing results of ANDs, and vice
      versa:
      #+BEGIN_SRC
      A AND (B OR C) = (A AND B) OR (A AND C).
      A OR (B AND C) = (A OR B) AND (A OR C).
     #+END_SRC
  + *DeMorgan's Law*
    * It can’t be summer and winter at once, so it’s either not summer
      or not winter. And it’s not summer and not winter if and only if
      it’s not the case it’s either summer or winter. Following this
      reasoning, ANDs can be transformed into ORs and vice versa:
      #+BEGIN_SRC
      !(A AND B) = !A OR !B,
      !A AND !B = !(A OR B).
      #+END_SRC
* Probability
  + *Independent Events*: When the outcome of an event does not
    influence the outcome of another event, they are independent. The
    probability that two independent events will happen is the product
    of their individual probabilities.

  + *mutually exclusive*: When two events cannot happen
    simultaneously, they are mutually exclusive. If you need any of
    the mutually exclusive events to happen, just sum their individual
    probabilities.
  + When two mutually exclusive events cover all possible outcomes,
    they are complementary. The sum of individual probabilities of
    complementary events is thus 100%.
* Complexity
  :LOGBOOK:
  - Note taken on [2018-11-18 Sun 19:38] \\
    You can drop bases of logarithms, when thinking asymptotically. log
    base 2 of n is equal to log base 10 of n when thinking asymptotically.
  - Note taken on [2018-11-18 Sun 17:37] \\
    - f(n) = O(g(n)) means f(n) and g(n) grow in same way as their input
      grows.
    - Big O class captures the *rate of growth* of two functions
    - we say two functions are in the same *big O class* if they have the
      same rate of growth.
  - Note taken on [2018-11-18 Sun 17:16] \\
    Why is asymptotic (asymptote: a straight line that continually
    approaches a given curve but does not meet it at any finite
    distance) analysis is useful? It calculates the big O class of
    code
  - Note taken on [2018-11-18 Sun 15:46] \\
    1. Count the number of operations
       What is an operation? a basic unit that doesn't change as the input changes
    2. Focus o how performance scales
    3. Go beyond input size
  :END:

- *Time complexity*: When an algorithm can have different values of =T(n)= for the same
  value of =n=, we resort to cases:

  1. *BEST CASE*: when the input requires the minimum number of
     operations for any input of that size. In sorting, it happens
     when the input is already sorted. It's the lower bound.

  2. *WORST CASE*: when the input requires the maximum number of
     operations for any input of that size. In many sorting
     algorithms, that’s when the input was given in reverse
     order. It's the upper bound.

  3. *AVERAGE CASE*: refers to the average number of operations required
     for typical inputs of that size. For sorting, an input in random
     order is usually considered.

  We can approximate T(n) by its fastest-growing term, called the
  *dominant term*.

  *EXAMPLES*:
  #+BEGIN_SRC java
    import java.util.Arrays;

    public class Library {
      public static void main(String[] args) {}

      // O(n)
      public static void reduce(int[] vals) {
        // O(1)
        int minIndex = 0;

        // O(n)
        for (int i = 0; i < vals.length; i += 1) {
          if (vals[i] < vals[minIndex]) {
            minIndex = i;
          }
        }

        // O(1)
        int minVal = vals[minIndex];

        // O(n)
        for (int i = 0; i < vals.length; i += 1) {
          vals[i] = vals[i] - minVal;
        }
      }

      // O(n^2)
      public static int maxDifference(int[] vals) {
        // O(1)
        int max = 0;

        // O(n^2)
        for (int i = 0; i < vals.length; i += 1) {
          // O(n)
          for (int j = 0; j < vals.length; j += 1) {
            // O(1)
            if (vals[i] - vals[j] > max) {
              max = vals[i] - vals[j];
            }
          }
        }

        // O(1)
        return max;
      }

      // O(n^2)
      // BTW the best case is also O(n^2)
      public static int[] selectionSort(int[] vals) {
        // O(n)
        for (int i = 0; i < vals.length; i += 1) {
          // O(n-i)
          for (int j = i + 1; j < vals.length; j += 1) {
            if (vals[j] < vals[i]) {
              int temp = vals[i];
              vals[i] = vals[j];
              vals[j] = temp;
            }
          }
        }

        return vals;
      }

      // HINT: start from the inside out
      // O(n) * O(1000) * O(n) => O(n^2)
      public static void challenge1(int n) {
        // O(1)
        int sum = 0;
        // O(2.n) => O(n) * times big O of the body
        for (int i = 0; i < 2 * n; i += 1) {
          // O(1000) times big O of the body
          for (int j = n - 1000; j < n; j += 1) {
            // O(n/2) => O(n) * times big O of the body
            for (int k = n / 2; k < n; k += 1) {
              // O(1)
              sum += 1;
            }
          }
        }
      }

      // log(n)
      public static boolean binarySearch(int[] arr, int toFind) {
        // O(1)
        var low = 0;
        var high = arr.length - 1;

        // log(n)
        while (low <= high) {
          var middle = (low + high) / 2;
          var value = arr[middle];

          if (value == toFind) {
            return true;
          } else if (toFind < value) {
            high = middle - 1;
          } else {
            low = middle + 1;
          }
        }

        return false;
      }

      // best case O(n)
      // worst case O(n^2)
      public static Integer[] insertionSort(Integer[] arr) {
        for (int i = 1; i < arr.length; i += 1) {
          int currInd = i;

          while (currInd > 0 && arr[currInd] < arr[currInd - 1]) {
            arr = swap(arr, currInd, currInd - 1);
            currInd = currInd - 1;
          }
        }
        return arr;
      }
      // O(n.log(n))
      public static Integer[] mergeSort(Integer[] arr) {
        if (arr.length == 1) {
          return arr;
        } else {
          return merge(
              mergeSort(Arrays.copyOfRange(arr, 0, arr.length / 2)),
              mergeSort(Arrays.copyOfRange(arr, arr.length / 2, arr.length)));
        }
      }

      // O(n)
      public static Integer[] merge(Integer[] firstArray, Integer[] secondArray) {
        ArrayList<Integer> result = new ArrayList<>();

        while (firstArray.length > 0 && secondArray.length > 0) {
          if (firstArray[0] < secondArray[0]) {
            result.add(firstArray[0]);
            firstArray = shift(firstArray);
          } else {
            result.add(secondArray[0]);
            secondArray = shift(secondArray);
          }
        }

        if (firstArray.length == 0) {
          for (Integer x : secondArray) {
            result.add(x);
          }
        } else if (secondArray.length == 0) {
          for (Integer x : firstArray) {
            result.add(x);
          }
        }

        return result.toArray(new Integer[result.size()]);
      }

      public static <T> T[] shift(T[] arr) {
        return Arrays.copyOfRange(arr, 1, arr.length);
      }

      public static <T> T[] swap(T[] arr, int firstIndex, int secondIndex) {
        T temp = arr[firstIndex];
        arr[firstIndex] = arr[secondIndex];
        arr[secondIndex] = temp;

        return arr;
      }

    }

  #+END_SRC

  *QuickSort*
  best case is =O(n.log(n))=
  average case is =O(n.log(n))=
  worst case is =O(n^2)=
  #+BEGIN_SRC java
    import java.util.Arrays;

    public class Library {
      public static void main(String args[]) {
        int[] sample = {9, 7, 5, 11, 12, 2, 14, 3, 10, 6};
        System.out.println(Arrays.toString(sample));
        quickSort(sample);
        System.out.println(Arrays.toString(sample));
      }

      public static int[] quickSort(int[] arr) {
        return quickSort(arr, 0, arr.length - 1);
      }

      public static int[] quickSort(int[] arr, int left, int right) {
        if (left >= right) {
          return arr;
        }

        var pivot = arr[(right + left) / 2];
        var index = partition(arr, left, right, pivot);

        quickSort(arr, left, index - 1);
        quickSort(arr, index + 1, right);

        return arr;
      }

      // by the end of this function, the given pivot will be in its final
      // position, and we return its position (left)
      public static int partition(int[] arr, int left, int right, int pivot) {
        while (left < right) {

          // Please find something that should be on the right side of our pivot,
          // that is to say it is bigger than the pivot
          while (left < (arr.length - 1) && arr[left] < pivot) {
            left += 1;
          }

          // Please find something that should be on the left side of our pivot,
          // that is to say it is smaller than the pivot
          while (right > 0 && arr[right] > pivot) {
            right -= 1;
          }

          if (left < right) {
            // we found two elment that should be swapped because,
            swap(arr, left, right);
          }
        }

        // position of pivot
        return left;
      }

      public static int[] swap(int[] arr, int firstIndex, int secondIndex) {
        var temp = arr[firstIndex];
        arr[firstIndex] = arr[secondIndex];
        arr[secondIndex] = temp;

        return arr;
      }
    }

  #+END_SRC

  *Binary Search recursively*
  =O(log n)=
  #+BEGIN_SRC java
    public class BinarySearch {
      public static void main(String[] args) {
        int[] sample = {1, 20, 22, 23, 24, 50, 55, 100};

        System.out.println(binarySearch(sample, 24));
      }

      public static boolean binarySearch(int[] arr, int toFind) {
        return binarySearch(arr, toFind, 0, arr.length - 1);
      }

      public static boolean binarySearch(int[] arr, int toFind, int low, int high) {
        if (low > high) {
          return false;
        }

        var midIndex = (low + high) / 2;
        var mid = arr[midIndex];

        if (mid > toFind) {
          return binarySearch(arr, toFind, low, midIndex - 1);
        } else if (mid < toFind) {
          return binarySearch(arr, toFind, midIndex + 1, high);
        } else {
          return true;
        }
      }
    }
  #+END_SRC
- *Space complexity*: The measure for the working storage an algorithm
   needs is called space complexity. Space complexity analysis is
   similar to time complexity analysis. The difference is that we
   count computer memory, and not computing operations.

* Strategy
  + *iteration*
  + *nested loops*
  + *recursion*
  + *brute force*
  + *backtracking*
    Backtracking works best in problems where the solution is a se-
    quence of choices and making a choice restrains subsequent choices.
    It identifies as soon as possible the choices you’ve made cannot give
    you the solution you want, so you can sooner step back and try
    something else. Fail early, fail often.
  + *heuristic*
    A heuristic method, or simply a heuristic, is a method that leads
    to a solution without guaranteeing it is the best or optimal one.
    * greedy
  + Divide and Conquer
    Problems with optimal substructure can be divided into similar but
    smaller subproblems.
  + *Dynamic Programming*
  + *Branch and Bound*
    * Upper and Lower Bounds
      1. Divide the problem into subproblems,
      2. Find upper and lower bounds of each new subproblem,
      3. Compare subproblem bounds of all branches,
      4. Return to step 1 with the most promising subproblem.
    With branch and bound, we predict which paths are worst and we
    avoid wasting energy exploring them.
* Data
** Abstractions
*** Data Types
    In computer science and computer programming, a data type or
    simply type is an attribute of data which tells the compiler or
    interpreter how the programmer intends to use the data. Most
    programming languages support common data types of real,
    integer and boolean. A data type constrains the values that an
    expression, such as a variable or a function, might take. This
    data type defines the operations that can be done on the data,
    the meaning of the data, and the way values of that type can be
    stored. A type of value from which an expression may take its
    value.
*** Abstract Data Types
   An Abstract Data Type (ADT) is the specification of a group of
   operations that make sense for a given data type.  In computer
   science, an abstract data type (ADT) is a mathematical model for
   data types, where a data type is defined by its behavior
   (semantics) from the point of view of a user of the data,
   specifically in terms of possible values, possible operations on
   data of this type, and the behavior of these operations. This
   contrasts with data structures, which are concrete representations
   of data, and are the point of view of an implementer, not a user.
   An Abstract Data Type only describes how variables of a given data
   type are operated. It provides a list of operations, but doesn’t
   explain how data operations happen.
****  Common Abstractions
    - *Primitive Data Types*:
    - *Stack*:
      + =push(e)=
      + =pop()=
    - *Queue*:
      + =enqueue(e)=: add an item e to the back of the queue
      + =dequeue()=: remove the item at the front of the queue
    - *The Priority Queue*: The Priority Queue is similar to the Queue,
      with the difference that enqueued items must have an assigned
      priority.
      + =enqueue(e, p)= : add an item =e= to the queue according to the priority level =p=,
      + =dequeue()= : remove the item at the front of the queue and return it.
    - *List*
      + =insert(n, e)=: insert the item e at position n
      + =remove(n)=: remove the item at position n
      + =get(n)=: get the item at position n
      + =sort()=: sort the items in the list
      + =slice()=: start, end : return a sub-list slice starting at the position start up until the position end
      + =reverse()= : reverse the order of the list
    - *The Sorted List*: The Sorted List is useful when you need to
      maintain an always sorted list of items.
      + =insert(e)=: insert item e at the right position in the list
      + =remove(n)=: remove the item at the position n in the list
      + =get(n)=: get the item at position n
    - *Map*: The Map (aka Dictionary) is used to store mappings
      between two objects: a key object and a value object. You can
      query a map with a key and get its associated value.
      + =set(key, value)= : add a key-value mapping,
      + =delete(key)= : remove key and its associated value,
      + =get(key)= : retrieve the value that was associated to key.
    - *Set*: The Set represents unordered groups of unique items, like
      mathematical sets. They’re used when the order of items you need
      to store is meaningless, or if you must ensure no items in the
      group occurs more than once.
      + =add(e)=: add an item to the set or produce an error if the item is already in the set
      + =list()=: list the items in the set
      + =delete(e)=: remove an item from the set
** Structures
   Behind the scene :) data structures describe how data is to be
   organized and accessed in the computer’s memory. They provide ways
   for implementing ADTs in data-handling modules.
*** Array
    marks the end of the sequence with a special =NULL= token.
*** Linked List
    A cell with an empty pointer marks the end of the
    chain. if we’re only given the address of a single cell, it’s
    not easy to remove it or move backwards.
*** The Double Linked List
    The Double Linked List is the Linked
    List with an extra: cells have two pointers: one to the cell
    that came before it, and other to the cell that comes after. And
    if we’re only given the address of a single cell, we’re able to
    delete it.
    :NOTE:
     - Linked Lists are preferable to Arrays when:
       + You need insertions/deletions in the list to be extremely fast,
       + You don’t need random, unordered access to the data,
       + You insert or delete items in the middle of a list,
       + You can’t evaluate the exact size of the list (it needs to
         grow or shrink throughout the execution).
     - Arrays are preferable over Linked Lists when:
       + You frequently need random, unordered access to the data,
       + You need extreme performance to access the items,
       + The number of items doesn’t change during execution, so you
         can easily allocate contiguous space of computer memory.
    :END:
*** Tree
- Trees are dynamic data structures
 - Like the Linked List, the Tree employs memory cells that do not
   need to be contiguous in physical memory to store objects.
 - Cells also have pointers to other cells. Unlike Linked Lists, cells
   and their pointers are not arranged as a linear chain of cells, but
   as a tree-like structure.
 - Trees are especially suitable for hierarchical data, such as a file
   directory structure.
 - Apart from the Root Node, nodes in trees must have exactly one
   parent
 - In the Tree terminology:
   + a cell is called a *node*
   + a pointer from one cell to another is called an *edge*
   + the topmost node of a tree is the *Root Node*: the only node that
     doesn’t have a parent
   + A node’s parent, grandparent, great-grandparent (and so on all
     the way to the Root Node) constitute the node’s *ancestors*
   + a node’s children, grandchildren, great-grandchildren (and so on
     all the way to the bottom of the tree) are the node’s *descendants*
   + Nodes that do not have any children are *leaf nodes*
   + And a *path* between two nodes is a set of nodes and edges that
     can lead from one node to the other
   + A node’s *level* is the size of its path to the Root Node
   + The tree’s *height* is the level of the deepest node in the tree
   + a set of trees can be referred to as a *forest*
**** Preorder Traversal
     This is a recursive process.
     This is depth first traversal.
     1. visit yourself
     2. then visit all your left subtree
     3. then visit all you right subtree
        #+BEGIN_SRC java
          public class BinaryTree<E> {
            TreeNode<E> root;

            private void preOrder(TreeNode<E> node) {
              if (node != null) {
                node.visit();
                preOrder(node.getLeftChild());
                preOrder(node.getRightChild());
              }
            }

            public void preOrder() {
              this.preOrder(root);
            }
          }
        #+END_SRC
**** Post Order Traversal
     This is depth first traversal
     1. then visit all your left subtree
     2. then visit all you right subtree
     3. visit yourself
**** In Order Traversal
     1. then visit all your left subtree
     2. visit yourself
     3. then visit all you right subtree
**** Level Order Traversal
     This is a breadth first traversal
     We want to visit in order: A B C D E F G
     #+BEGIN_SRC
             A
           /   \
          B     C
         / \   /  \
        D   E F    G
     #+END_SRC
     #+BEGIN_SRC java
       public void levelOrder() {
         Queue< TreeNode<E> > q = new LinkedList< TreeNode<E> >();
         q.add(this.root);

         while(!q.isEmpty()) {
           TreeNode<E> curr = q.remove();

           if(curr != null) {
             curr.visit();
             q.add(curr.getLeftChild());
             q.add(curr.getRightChild());
           }
         }
       }
     #+END_SRC
**** Tree Balancing
 If we insert too many nodes in a Binary Search Tree, we end up with a
 tree of very high height, where many nodes have only one child. But
 we can rearrange nodes in a tree such that its height is
 reduced. This is called tree balancing. A perfectly balanced tree has
 the minimum possible height.

 Most operations with trees involve following links between nodes
 until we get to a specific one. The higher the height of the tree,
 the longer the average path between nodes, and the more times we need
 to access the memory. Therefore, it’s important to reduce tree
 height.

 In a balanced Tree:
 #+BEGIN_SRC
 |leftHight - rightHeight| <= 1
 #+END_SRC
 the height of a balanced Tree is around =log(n)=

 for a word search function say =isWord(String wordToFind)= the time complexity is

 |              | Best Case | Average Case | Worst Case |
 |--------------+-----------+--------------+------------|
 | linked list  | O(1)      | O(n)         | O(n)       |
 | BST          | O(1)      | O(log n)     | O(n)       |
 | Balanced BST | O(1)      | O(log n)     | O(log n)   |
 |              |           |              |            |

 #+BEGIN_SRC
 4                             6                         10
  \                           /  \                     /    \
   6                         4    8                   6      18
    \                              \                 / \    /  \
     8                              10              4   8  15   21
      \                              \
       10                             18
        \                            /  \
         18                         15   21
        /  \
       15   21
 #+END_SRC

 #+BEGIN_SRC
 function build_balanced nodes
   if nodes is empty
      return NULL

   middle ← nodes.length/
   left ← nodes.slice(0, middle - 1)
   right ← nodes.slice(middle + 1, nodes.length)
   balanced ← BinaryTree.new(root=nodes[middle])
   balanced.left ← build_balanced(left)
   balanced.right ← build_balanced(right)

   return balanced
 #+END_SRC

**** Types of Trees
***** Self Balancing Trees
To efficiently handle binary trees that change a lot, selfbalancing
binary trees were invented. Their procedures for inserting or
removing items directly ensure the tree stays balanced.

- *The Red-Black Tree*: The Red-Black Tree is a famous example of a
  self-balancing tree, which colors nodes either “red” or “black” for
  its balancing strategy. Red-Black Trees are frequently used to
  implement Maps: the map can be heavily edited in an efficient way,
  and finding any given key in the map remains fast because of
  self-balancing.

- *AVL Tree*:The AVL Tree is another breed of self-balancing
  trees. They require a bit more time to insert and delete items than
  Red-Black Trees, but tend to have better balancing. This means
  they’re faster than Red-Black Trees for retrieving items. AVL Trees
  are often used to optimize performance in read-intensive scenarios.
- *B-Tree*:
***** Binary Search Tree
 A Binary Search Tree is a special type of Tree that can be
 efficiently searched. Nodes in Binary Search Trees can have at most
 two children. And nodes are positioned according to their
 value/key. Children nodes to the left of the parent must be smaller
 than the parent, children nodes to the right must be greater.

 *Structure(shape) of BST depends on the order of insertion*.

 #+BEGIN_SRC
               X
              / \
             Y   Z

         Y <= X; Z >= X
 #+END_SRC

- Searching
 #+BEGIN_SRC
 function find_node(binary_tree, value)
   node ← binary_tree.root_node

   while node
       if node.value = value
           return node
       if value > node.value
           node ← node.right
       else
           node ← node.left
   return "NOT FOUND"
 #+END_SRC
 in Java
 #+BEGIN_SRC java
   import java.util.LinkedList;
   import java.util.Queue;

   public class BinaryTree<E extends Comparable<? super E>> {
     private TreeNode<E> root;

     public boolean contains(E toFind) {
       TreeNode<E> curr = this.root;
       int comp;

       while (curr != null) {
         comp = toFind.compareTo(curr.value());

         if (comp < 0) {
           curr = curr.getLeftChild();
         } else if (comp > 0) {
           curr = curr.getRightChild();
         } else {
           return true;
         }
       }

       return false;
     }
   }

 #+END_SRC

- Inserting
 #+BEGIN_SRC
 function insert_node(binary_tree, new_node)
   node ← binary_tree.root_node

   while node
       last_node ← node

       if new_node.value > node.value
           node ← node.right
       else
           node ← node.left

   if new_node.value > last_node.value
       last_node.right ← new_node
   else
       last_node.left ← new_node
 #+END_SRC

 in Java
 #+BEGIN_SRC java
   import java.util.LinkedList;
   import java.util.Queue;

   public class BinaryTree<E extends Comparable<? super E>> {
     private TreeNode<E> root;

     public boolean insert(E toInsert) {
       TreeNode<E> curr = this.root;
       int comp = toInsert.compareTo(curr.value());

       while (comp < 0 && curr.getLeftChild() != null || comp > 0 && curr.getRightChild() != null) {
         if (comp < 0) curr = curr.getLeftChild();
         else curr = curr.getRightChild();

         comp = toInsert.compareTo(curr.value());
       }
       comp = toInsert.compareTo(curr.value());

       if (comp < 0) {
         curr.addLeftChild(toInsert);
       } else if (comp > 0) {
         curr.addRightChild(toInsert);
       } else {
         return false;
       }

       return true;
     }
   }
 #+END_SRC
- Deleting
  Scenarios:
  1. it's a leaf node
  2. it has one child
  3. it has two children

  courtesy of https://www.makeinjava.com/delete-remove-node-binary-search-tree-bst-java-dfs-example/
  #+BEGIN_SRC java
    public class Node {
      public int data;
      public Node left;
      public Node right;

      public Node(int num) {
        this.data = num;
        this.left = null;
        this.right = null;
      }

      public Node() {
        this.left = null;
        this.right = null;
      }

      public static Node createNode(int number) {
        return new Node(number);
      }
    }

    public class DeleteNodeInBST {
      public static void inorder(Node root) {
        if (root == null) return;
        inorder(root.left);
        System.out.printf("%d ", root.data);
        inorder(root.right);
      }

      private static int min(Node node) {
        if (node.left == null) {
          return node.data;
        }
        return min(node.left);
      }

      public static Node deleteNodeInBST(Node node, int data) {
        if (null == node) {
          System.out.println("Element is not there in binary search tree");
          return null;
        }
        if (data < node.data) {
          node.left = deleteNodeInBST(node.left, data);
        } else if (data > node.data) {
          node.right = deleteNodeInBST(node.right, data);
        } else { // case for equality
          // Now we see that whether we can directly delete the node
          // [Scenario 3]
          if (node.left != null && node.right != null) {
            int minInRightSubTree = min(node.right);
            node.data = minInRightSubTree;
            node.right = deleteNodeInBST(node.right, minInRightSubTree);
          } else { // either one child or leaf node
            // [Scenario 1 and Scenario 2]
            if (node.left == null && node.right == null) {
              node = null;
            } else { // one child case
              Node deleteNode = node;
              node = (node.left != null) ? (node.left) : (node.right);
              deleteNode = null;
            }
          }
        }
        return node;
      }
    }

  #+END_SRC
****** The Binary Heap
The Binary Heap is a special type of Binary Search Tree, in which we
can find the smallest (or highest) item instantly. This data structure
is especially useful for implementing Priority Queues. In the Heap it
costs =O(1)= to get the minimum (or maximum) item, because it is
always the Root Node of the tree. Searching or inserting nodes still
costs =O(log n)=. It has the same node placement rules as the Binary
Search Tree, plus an extra restriction: a parent node must be greater
(or smaller) than both its child nodes.

Use the Binary Heap whenever you must frequently
work with the maximum (or minimum) item of a set.


=MAX HEAP=
#+BEGIN_SRC
       21                       X
     /    \                    / \
    8      18                 Y   Z
   / \    /  \
  4   6  10   15            X ⩾ Z ⩾ Y
#+END_SRC

=MIN HEAP=
#+BEGIN_SRC
       4                      X
     /   \                   / \
    6     8                 Y   Z
         / \              X ⩽ Z ⩽ Y
        10  15
           /  \
          18   21
#+END_SRC

***** Trie
Comes from the for *reTRIEval*, the pronunciation is different from
=tree= just to distinguish it.

- Nodes can have more than two children.



****** Performance
If there are n words in the dictionary, what is the worst case time to
find a word?


Let's start by a balanced BST. For a balanced BST, the number of words
in the dictionary determines the worst case scenario, which =O(log n)=.

#+BEGIN_SRC
                ear
               /   \
              /     \
             /       \
            at        east
           /  \       /
          /    \    eat
         /      \
        a       ate
#+END_SRC

But if we use a TRIE to store the dictionary it's the length of the
words that determines the worst case scenario. So the worst case
scenario is the length of the longest word in the dictionary.
*** Graph
The Graph is similar to the Tree. The difference is that there’s no
children or parent nodes, and therefore, no Root Node. Data is freely
arranged as nodes and edges, and any node can have multiple incoming
and outgoing edges. This is the most flexible data structure there
is, and it can be used to represent almost any type of data.
**** Searching in Graphs
***** DFS and BFS
Notice that DFS and BFS only differ in the way the next nodes to
explore are stored: one uses a Queue, the other a Stack.

When you suspect the node you are searching isn’t many levels away
from the start, it’s usually worth paying the higher cost of BFS,
because you’re likely to find the node faster. When you need to
explore all the nodes of a graph, it’s usually better to stick with
DFS for its simple implementation and smaller memory footprint.

pseudo-code
#+BEGIN_SRC
function DFS(start_node, key)
    next_nodes ← Stack.new()
    seen_nodes ← Set.new()

    next_nodes.push(start_node)
    seen_nodes.add(start_node)

    while not next_nodes.empty
        node ← next_nodes.pop()

        if node.key = key
            return node

        for n in node.connected_nodes
            if not n in seen_nodes
                next_nodes.push(n)
                seen_nodes.add(n)
    return NULL

/-------------

function BFS(start_node, key)
    next_nodes ← Queue.new()
    seen_nodes ← Set.new()

    next_nodes.enqueue(start_node)
    seen_nodes.add(start_node)

    while not next_nodes.empty
        node ← next_nodes.dequeue()
        if node.key = key
            return node
        for n in node.connected_nodes
            if not n in seen_nodes
               next_nodes.enqueue(n)
               seen_nodes.add(n)
    return NULL
#+END_SRC
Recursive and iterative version of DFS in Java, and iterative version BFS, courtesy of [[https://java2blog.com/depth-first-search-in-java/][java2blog]]
#+BEGIN_SRC java
  import java.util.ArrayList;
  import java.util.List;
  import java.util.Stack;

  public class Graph {

      static class Node {
          int data;
          boolean visited;
          List<Node> neighbours;

          Node(int data) {
              this.data = data;
              this.neighbours = new ArrayList<>();
          }

          public void addneighbours(Node neighbourNode) {
              this.neighbours.add(neighbourNode);
          }

          public List<Node> getNeighbours() {
              return neighbours;
          }

          public void setNeighbours(List<Node> neighbours) {
              this.neighbours = neighbours;
          }
      }

      // Recursive DFS
      public void dfs(Node node) {
          System.out.print(node.data + " ");
          List<Node> neighbours = node.getNeighbours();
          node.visited = true;
          for (int i = 0; i < neighbours.size(); i++) {
              Node n = neighbours.get(i);
              if (n != null && !n.visited) {
                  dfs(n);
              }
          }
      }

      // Iterative DFS using stack
      public void dfsUsingStack(Node node) {
          Stack<Node> stack = new Stack<Node>();
          stack.add(node);
          node.visited = true;
          while (!stack.isEmpty()) {
              Node element = stack.pop();
              System.out.print(element.data + " ");
              List<Node> neighbours = element.getNeighbours();
              for (int i = 0; i < neighbours.size(); i++) {
                  Node n = neighbours.get(i);
                  if (n != null && !n.visited) {
                      stack.add(n);
                      n.visited = true;
                  }
              }
          }
      }

      public void bfs(Node node) {
          queue.add(node);
          node.visited=true;
          while (!queue.isEmpty()) {
              Node element=queue.remove();
              System.out.print(element.data + "t");
              List<Node> neighbours=element.getNeighbours();
              for (int i = 0; i < neighbours.size(); i++) {
                  Node n=neighbours.get(i);
                  if(n!=null && !n.visited) {
                      queue.add(n);
                      n.visited=true;
                  }
              }

          }
      }

      public static void main(String arg[]) {

          Node node40 = new Node(40);
          Node node10 = new Node(10);
          Node node20 = new Node(20);
          Node node30 = new Node(30);
          Node node60 = new Node(60);
          Node node50 = new Node(50);
          Node node70 = new Node(70);

          node40.addneighbours(node10);
          node40.addneighbours(node20);
          node10.addneighbours(node30);
          node20.addneighbours(node10);
          node20.addneighbours(node30);
          node20.addneighbours(node60);
          node20.addneighbours(node50);
          node30.addneighbours(node60);
          node60.addneighbours(node70);
          node50.addneighbours(node70);

          Graph dfsExample = new Graph();

          System.out.println("The DFS traversal of the graph using stack ");
          dfsExample.dfsUsingStack(node40);

          System.out.println();

          // Resetting the visited flag for nodes
          node40.visited = false;
          node10.visited = false;
          node20.visited = false;
          node30.visited = false;
          node60.visited = false;
          node50.visited = false;
          node70.visited = false;

          System.out.println("The DFS traversal of the graph using recursion ");
          dfsExample.dfs(node40);
      }
  }

#+END_SRC

**** Graph Coloring
**** Path Finding
To find short paths, BFS and DFS strategies are usable but bad.  One
famous and very effective way of finding the shortest path is the
*Djkistra Algorithm*: As BFS uses an auxiliary Queue to keep track of
nodes to explore, the Djkistra Algorithm uses a Priority Queue. When
new nodes are explored, their connections are added to the Priority
Queue. A node’s priority is the weight of the edges that take it to
the starting node. This way, the next node to explore is always the
closest to where we started.
**** PageRank
**** Operation Research
Operations research involves defining an objective to maximize or
minimize. It can help maximize objectives like yield, profit, or
performance; and minimize objectives like loss, risk, or cost.
**** Linear Optimization problem
Problems where the objective and constraints can be modeled using
linear equations (Formally, polynomials with degree 1. They can have
no squares (nor any powers) and their variables can only be multiplied
by constant numbers.) are called linear optimization problems.
Can be solved by Simplex Method.

*** HASHTABLE
    GOOD: AVERAGE =O(1)= LOOKUP, INSERT, REMOVE.
    BAD: RESIZING COST, NO DATA ORDERING.

- THE HASH TABLE (THEY ARE JUST ARRAYS AT THE CORE) IS A DATA
  STRUCTURE THAT ALLOWS FINDING ITEMS IN =O(1)= TIME.
- SIMILARLY TO THE ARRAY, THE HASH REQUIRES PREALLOCATING A BIG CHUNK
  OF SEQUENTIAL MEMORY TO STORE DATA.
- BUT UNLIKE THE ARRAY, ITEMS ARE NOT STORED IN AN ORDERED
  SEQUENCE. THE POSITION AN ITEM OCCUPIES IS GIVEN BY A
  HASH FUNCTION.
- *HASH FUNCTION*: A SPECIAL FUNCTION THAT TAKES THE DATA YOU WANT TO
  STORE AS INPUT, AND OUTPUTS A RANDOM-LOOKING NUMBER. THAT NUMBER IS
  INTERPRETED AS THE MEMORY POSITION THE ITEM WILL BE STORED AT. THIS
  ALLOWS US TO RETRIEVE ITEMS INSTANTLY. A GIVEN VALUE IS FIRST RUN
  THROUGH THE HASH FUNCTION. THE FUNCTION WILL OUTPUT THE EXACT
  POSITION THE ITEM SHOULD BE STORED IN MEMORY. FETCH THAT MEMORY
  POSITION. IF THE ITEM WAS STORED, YOU’LL FIND IT THERE.
- *HASH COLLISION*: A PROPER HASH FUNCTION WILL RETURN RANDOM-LOOKING
  VALUES FOR DIFFERENT INPUTS. THEREFORE, THE LARGER THE RANGE OF
  VALUES THE HASH FUNCTION CAN OUTPUT, THE MORE DATA POSITIONS ARE
  AVAILABLE, AND THE LESS PROBABLE IT IS FOR A HASH COLLISION TO
  HAPPEN. SO WE ENSURE AT LEAST 50% OF THE SPACE AVAILABLE TO THE HASH
  TABLE IS FREE. OTHER- WISE, COLLISIONS WOULD BE TOO FREQUENT,
  CAUSING A SIGNIFICANT DROP IN THE HASH TABLE’S PERFORMANCE. HASH
  TABLES ARE OFTEN USED TO IMPLEMENT MAPS AND SETS. THEY ALLOW FASTER
  INSERTIONS AND DELETIONS THAN TREE-BASED DATA STRUCTURES. HOWEVER,
  THEY REQUIRE A VERY LARGE CHUNK OF SEQUENTIAL MEMORY IN ORDER TO
  WORK PROPERLY.
  + SOLUTIONS:
    1. *LINEAR PROBING*: JUST PUT IT IN THE NEXT OPEN SPOT. LINEAR
       PROBING CAN STRUGGLE AS THE HASH TABLE STARTS GETTING
       FULL. WHEN WE WANT TO SEARCH FOR A KEY, WE HAVE TO KEEP
       LOOKING UNTIL WE GET TO AN OPEN SPOT.

       + *RANDOM PROBING*: JUMP RANDOM # OF STEPS INSTEAD OF JUST THE NEXT ONE
    2. *SEPARATE CHAINING*: JUST KEEP A LINKED LIST AT EACH SPOT. WHEN
       COLLISION HAPPENED ADD THE ELEMENT TO THE LIST.
- *RESIZING*:
  + REQUIRES TO CREATE A NEW TABLE, NEW HASH FUNCTION AND REINSERTING
    EVERYTHING (AT THE CORE, THEY ARE JUST ARRAYS).
  + RULE OF THUMB: TOO FULL IS ≈ 70% FULL
- *ORDERING*:
  + NO ORDER WITHIN THE STRUCTURE ITSELF.
* Database
** Relational
   Check out [[file:sql.org][sql.org]]
** Non-Relational
*** document stores
- In document stores, data entries are kept exactly the way they are
  needed by the application.
- The non-relational model expects us to duplicate at information each
  relevant place.
- There are no “tables” and “rows” in document stores. Instead, a data
  entry is called a document. Related documents are grouped in a
  collection.
*** Key-value stores
*** Graph databases
- In a graph database, data entries are stored as nodes, and rela-
  tionships as edges. Nodes are not tied to a fixed schema and can
  store data flexibly.
*** Big Data
Big Data describes data-handling situations that are extremely
challenging in terms of Volume, Velocity, or Variety.

- *VOLUME* means you handle thousands of terabytes, as is the case of
  the LHC (Large Hadron Collider).
- *VELOCITY* means you must store mil- lion of writes per second
  without holdups, or serve billions of read queries quickly.
- Data *VARIETY* means that the data doesn’t have a strong structure,
  so it’s difficult to handle it using traditional relational
  databases.

** Distributed
There are several situations in which not one, but several computers
must act in coordination to provide a database system.
- Databases of several hundred terabytes. Finding a single computer
  with that much storage space is impractical.
- Database systems that process several thousand simultaneous queries
  per second. No single computer has enough networking or processing
  power to handle such a load.
- Mission-critical databases, such as the ones recording altitude
  and speed of aircraft currently in a given airspace. Relying on a
  single computer is too risky—if it crashes, the database becomes
  unavailable.

*** Single-Master Replication
One computer is the master and receives all queries to the
database. It is connected to several other slave computers. Each slave
has a replica of the database. As the master receives write queries,
it forwards them to slaves, keeping them synchronized:


#+BEGIN_SRC
read query-----------↓
                     ↓  |----------------->  ___
                     ↓  |                   |   |
                     ↓  |                   |___| (slave)
                     ___                      ╧
                    |   |  ---------------> ___
           (master) |___|                  |   | (slave)
                      ╧                    |___|
                     ↑  |                    ╧
                     ↑  |                   ___
                     ↑  |--------------->  |   |
write query----------↑                     |___| (slave)
                                             ╧
#+END_SRC

With this setup, the master is able to serve more read queries,
because it can delegate those to the slaves. And the system becomes
more reliable: if the master computer shuts down, the slave ma- chines
can coordinate and elect a new master automatically. That way, the
system doesn’t stop running.

*** Multi-Master Replication
If your database system must support a massive amount of simultaneous
write queries, a single master cannot handle all the load. In this
case, all computers in the cluster become masters. A load balancer is
used to distribute incoming read and write queries equally among the
machines in the cluster.

Each computer is connected to all others in the cluster. They
propagate write queries among themselves, so they all remain
synchronized. Each has a copy of the entire database.
*** Sharding
If your database receives many write queries for large amounts of
data, it’s hard to synchronize the database everywhere in the cluster.
Some computers might not have enough storage space to accommodate the
entire thing. One solution is to partition the database among the
computers. Since each machine owns a portion of the database, a query
router forwards queries to the relevant one.

#+BEGIN_SRC
read query-----------↓
                     ↓  |----------------->  ___ (shard #1 from A-I)
                     ↓  |                   |   |
                     ↓  |                   |___|
                     ___                      ╧
                    |   | --------------->  ___
     (query router) |___|                  |   | (shard #2 from I-R)
                                           |___|
                     ↑  |                    ╧
                     ↑  |                   ___
                     ↑  |--------------->  |   |
write query----------↑                     |___| (shard #3 from R-Z)
                                             ╧
#+END_SRC
This setup can process many read and write queries for super huge
databases. But it has a problem: if a machine in the cluster fails, the
parts of data it is responsible for become unavailable. To mitigate
that risk, sharding can be used with replication:
*** Data Consistency
In distributed databases with replication, updates made in one machine
don’t propagate instantly across all replicas. It takes some time
until all machines in the cluster are synchronized. That can damage
the consistency of your data.

There is a trade-off between consistency and performance. If your
database queries do not strongly enforce data consistency, they are
said to work under *eventual consistency*. Data is guaranteed to
eventually be consistent, after some time. This means some write
queries might not be applied, and some read queries may return
out-of-date information.
*** Geographical
*** Serialization formats
How can we store data outside of our database, in a format that
is interoperable across different systems? For instance, we might
want to backup the data, or export it to an other system. To do this,
the data has to go through a process called serialization, where
it is transformed according to an encoding format. The resulting
file can be understood by any system that supports that encoding
format.
- Encoding formats:
  1. SQL: SQL is the most common format for serializing relational
     databases. We write a series of SQL commands that replicate the
     database and all its details. Most relational database systems
     include a “dump” command to create an SQL-serialized file of your
     database. They also include a “restore” command to load such a
     “dump file” back to the database system.
  2. XML
  3. JSON
  4. CSV
* Computers
- The memory can operate in read or write mode.
- Each memory cell stores an 8-digit binary number, which is called a
  byte. In “read” mode, the memory retrieves the byte stored in a
  cell, and outputs it in eight data-transmitting wires.
- When the memory is in “write” mode, it gets a byte from these wires,
  and writes it to the informed cell.
- A group of wires used for transmitting the same data is a *bus*. The
  eight wires used to transmit addresses form the *address bus*. The
  other eight wires used to transmit data to and from memory cells
  form the *data bus*. While the address bus is unidirectional (only
  used to receive data), the data bus is bidirectional (used to send
  and receive data).

- The collection of all operations a CPU can do is called its
  *instruction set*. Each operation in the instruction set is assigned
  a number. Computer code is essentially a sequence of numbers
  representing CPU operations. These operations are stored as numbers
  in the RAM. We store input/output data, partial calculations, and
  computer code, all mixed together in the RAM.
- The CPU works in a never-ending cycle, always fetching and executing
  an instruction from memory. At the core of this cycle is the PC
  register, or *Program Counter*. It’s a special register that stores
  the memory address of the next instruction to be executed. The CPU
  will:
  1. Fetch the instruction at the memory address given by the PC (Program Counter),
  2. Increment the PC by 1,
  3. Execute the instruction,
  4. Go back to step 1.
- *CPU Clock*: a number that indicates the CPU’s clock: the number of
  basic operations it executes per second. A machine instruction
  requires five to ten basic operations to complete.
- *CPU architecture*: A different CPU architecture means a different
  CPU instruction set, thus a different way to encode in- structions
  as numbers. Numbers that translate as instructions for your desktop
  CPU don’t represent valid instructions for the CPU in your cell
  phone, and vice versa.
- *32 bit VS. 64 bit architecture*: The first CPU, called Intel 4004,
  was built on a 4-bit architecture. This means it could operate (sum,
  compare, move) binary numbers of up to 4 digits in a single machine
  instruction. The 4004 had data and address buses with only four
  wires each.  Larger registers naturally lead to larger data and
  address buses. An address bus with 32 wires allows addressing 2^32
  bytes (4 GB) of memory.
- *BIG-ENDIAN VS. LITTLE ENDIAN*: Some computer designers thought it made
  sense to store numbers left-to-right in the RAM and CPU, in a way
  that is known as little-endian. Other computer designers preferred
  to write data in memory right-to-left, in what is known as
  big-endian. The binary sequence 1-0-0-0-0-0-1-1 can represent
  different numbers, depending on “endianness”:
  + Big-endian: 2^7 + 2^1 + 2^0 = 131,
  + Litte-endian: 2^0 + 2^6 + 2^7 = 193.
** compilers
- we rarely write our programs directly as CPU instructions. It would
  be impossible for a human to write a realistic 3D computer game this
  way. To express our orders in a more “natural” and compact way, we
  created programming languages. We write our code in these
  languages.9 Then, we use a program called a compiler to translate
  our orders as machine instructions a CPU can run.

- Alan Turing, the father of computing, discovered that simple
  machines can be powerful enough to compute anything that is com-
  putable. For a machine to have universal computing power, it must be
  able to follow a program containing instructions to:
  + Read and write data in memory,
  + Perform conditional branching: if a memory address has a given
    value, jump to another point in the program.
  Machines that have such universal computing power are called *turing-complete*.
** Operation systems
A compiled program may not be usable on two computers that share the
same CPU architecture. That’s because programs must communicate with
the computer’s operating system to run.  To communicate with the
world, programs must input and output stuff: open files, write a
message on the screen, open a network connection, etc. But different
computers have different hardware.  It’s impossible for a program to
directly support all different types of screens, sound cards, or
network cards.  That’s why programs rely on an operating system to
execute.  With its help, programs can work effortlessly with different
hardware. Programs make special *system calls*, requesting the
operating system to perform required input/output
operations. Compilers translate input/output commands into the
appropriate system calls.  However, different operating systems often
use incompatible system calls. The system call for printing something
on the screen with Windows is different from the one used by Mac OS,
or Linux.  That’s why if you compile your program to run on Windows
with a x86 processor, it won’t work on a Mac with a x86 processor.
Besides targeting a specific CPU architecture, compiled code also
targets a specific operating system.
** Memory Hierarchy
We know a computer works by having its CPU execute simple
instructions. We know these instructions can only operate on data
stored in CPU registers. However, their storage space is usually
limited to less than a thousand bytes. This means CPU registers
constantly have to transfer data to and from the RAM.

** Process-Memory Gap
In modern computers, it takes about a thousand CPU cycles to get data
from the RAM—about 1 microsecond.13 That’s incredibly fast, but it’s
an eternity compared to the time it takes to access CPU
registers. Computer scientists started to try to find ways to reduce
the number of RAM operations required by their computations.

*** Temporal and Spatial Locality
When trying to minimize RAM access, computer scientists started
noticing two things:

- *Temporal Locality*: when a memory address is accessed, it’s probable
  it will be accessed again soon.
- *Spatial Locality* : when a memory address is accessed, it’s probable
  addresses that are adjacent to it are going to be accessed soon.
**** L1 Cache
Getting data from this memory into the registers is just a tiny bit
slower than getting data from the registers themselves.

With an L1 cache, we can copy the contents of memory addresses with
high probability of being accessed close to the CPU registers. This
way, they can be very quickly loaded into CPU reg- isters. It takes
only about ten CPU cycles to get data from the L1 cache into the
registers. That’s about a hundred times faster then fetching it from
the RAM.
**** The L2 Cache
Increasing the size of the L1 cache would make fetching data from the
RAM an even rarer operation, further reducing CPU waiting
time. However, growing the L1 cache without slowing it down is
difficult. After the L1 cache is about 50 KB in size, further
increasing it gets very expensive. The better solution is to build an
additional memory cache: the L2 cache. By allowing it to be slower, it
can be much larger than the L1 cache. A modern CPU will have about 200
KB of L2 cache. It takes about a hundred CPU cycles to get data from
the L2 cache into CPU registers.

| memory | speed  |
|--------+--------|
| CPU    | 1ns    |
| L1     | 10ns   |
| L2     | 100ns  |
| RAM    | 1000ns |
| DISK   | 1ms    |
* Programming
** Linguistics
Programming languages differ wildly, but all of them exist to do
one thing: manipulate information. These languages rely on three
basic building blocks to do this.
1. A =value= represents information
2. An =expression= produces a value.
   - You can create a value in two ways: either by writing a literal,
     or by calling a function.
   -  In the end, even the most complex expression must always
     evaluate to a single value.
3. A =statement= uses a value to give an instruction to a computer
** Paradigms
A paradigm is a specific collection of concepts and practices that
define a field of science. A paradigm will orientate how you approach
a problem, the techniques you use, and the structure of your solution.

*** programming paradigms
**** Imperative
- The imperative programming paradigm is about instructing the
  computer on what it must do exactly at each step using specific
  commands.
- Each command changes the computer’s state. The se- quence of
  commands that constitutes a program is followed, one after another.
- *Machine Programming*: Early programmers, who had to input their
  code manually to the computer using 1s and 0s, also got lazy. They
  decided it would be more fun to write their sequence of CPU
  instructions using mnemonics, such as CP for the “copy” instruction,
  MOV for the “move” instruction, CMP for the “compare” instruction,
  and so on. Then they wrote a program that converted mnemonic code to
  equivalent binary numbers, which could then be executed by the
  computer. With this, the Assembly (aka ASM) language was born.
- *Structured Programming*: In the beginning, programs used GOTO
  commands to control the execution flow. This command makes the
  execution to jump to a different part of the code.
- *Procedural programming*: The next advance in the art of coding was
  procedural programming. It allows code to be organized into
  procedures, to avoid code replication and to increase code
  reusability.

**** Declarative
The declarative programming paradigm makes you state your desired
result without dealing with every single intricate step that gets you
there. It’s about declaring what you want, and not how you want it
done. In many scenarios, this allows programs to be much shorter and
simpler. They are often also easier to read.

- *Functional Programming*: In the functional programming paradigm,
  functions are more than just procedures. They’re used to declare the
  relationship between two or more items, much like math equa-
  tions. Functions are first-class citizens in the functional
  paradigm. They are treated the same way as any other primitive data
  type, such as strings and numbers.

**** Logic
Whenever your problem is the solution to a set of logical formulas,
you can use logic programming.
